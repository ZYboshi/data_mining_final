# data_process.py
"""
    æ•°æ®å¤„ç†ï¼š
    1. è·å–æ•°æ®é›†
    2. å¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†
    3. ä¿å­˜å¤„ç†åçš„æ•°æ®
"""
import json
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
import warnings
import os   
warnings.filterwarnings('ignore')

def download_dataset():
    """
    ä¸‹è½½æ•°æ®é›†ï¼ˆå¦‚æœæœªä¸‹è½½ï¼‰
    """
    import os
    from sklearn.datasets import fetch_openml
    
    # åˆ›å»º dataset æ–‡ä»¶å¤¹ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    if not os.path.exists('./dataset'):
        os.makedirs('./dataset')
    
    # ä¸‹è½½æ•°æ®é›†
    bank = fetch_openml(name='bank-marketing', version=1, as_frame=True)
    
    # å°†æ•°æ®ä¿å­˜ä¸º CSV æ–‡ä»¶
    data_path = './dataset/bank_marketing.csv'
    bank.frame.to_csv(data_path, index=False)
    
    print(f"Data saved to {data_path}")
    return data_path

def load_and_rename_data(file_path):
    """
    åŠ è½½æ•°æ®å¹¶é‡å‘½ååˆ—
    
    å‚æ•°:
    file_path: æ•°æ®æ–‡ä»¶è·¯å¾„
    
    è¿”å›:
    data: å¤„ç†åçš„DataFrame
    """
    # åŠ è½½æ•°æ®
    data = pd.read_csv(file_path)
    print("æ•°æ®å½¢çŠ¶:", data.shape)
    
    # å®šä¹‰åˆ—åæ˜ å°„
    column_names = {
        'V1': 'age',
        'V2': 'job',
        'V3': 'marital',
        'V4': 'education',
        'V5': 'default',
        'V6': 'balance',
        'V7': 'housing',
        'V8': 'loan',
        'V9': 'contact',
        'V10': 'day',
        'V11': 'month',
        'V12': 'duration',
        'V13': 'campaign',
        'V14': 'pdays',
        'V15': 'previous',
        'V16': 'poutcome',
        'Class': 'deposit'
    }
    
    # é‡å‘½ååˆ—
    data = data.rename(columns=column_names)
    print("é‡å‘½ååçš„åˆ—å:", list(data.columns))
    
    return data

def explore_data(data):
    """
    æ•°æ®æ¢ç´¢åˆ†æ
    
    å‚æ•°:
    data: è¦æ¢ç´¢çš„DataFrame
    
    è¿”å›:
    categorical_cols: ç±»åˆ«å‹åˆ—åˆ—è¡¨
    """
    # åŸºæœ¬ä¿¡æ¯
    print("\næ•°æ®ç±»å‹:")
    print(data.dtypes)
    
    print("\næ•°æ®ç»Ÿè®¡æè¿°:")
    print(data.describe())
    
    # ç±»åˆ«å‹å˜é‡
    categorical_cols = ['job', 'marital', 'education', 'default', 'housing', 
                        'loan', 'contact', 'month', 'poutcome', 'deposit']
    
    print("\nç±»åˆ«å‹å˜é‡ç»Ÿè®¡:")
    for col in categorical_cols:
        print(f"\n{col}:")
        print(data[col].value_counts())
    
    print("\nç¼ºå¤±å€¼ç»Ÿè®¡:")
    print(data.isnull().sum())
    
    return categorical_cols

def generate_column_info(data, save_path='./dataset/column_info.json'):
    """
    ç”Ÿæˆåˆ—ä¿¡æ¯å¹¶ä¿å­˜ä¸ºJSONæ–‡ä»¶
    
    å‚æ•°:
    data: è¾“å…¥çš„DataFrame
    save_path: JSONæ–‡ä»¶ä¿å­˜è·¯å¾„
    """
    data_info = {
        "dataset_shape": {
            "rows": int(data.shape[0]),
            "columns": int(data.shape[1])
        },
        "columns": {}
    }
    
    # ä¸ºæ¯ä¸€åˆ—æ”¶é›†ä¿¡æ¯
    for column in data.columns:
        col_type = str(data[column].dtype)
        
        # æ•°å€¼å‹åˆ—
        if col_type in ['int64', 'float64']:
            data_info["columns"][column] = {
                "type": "numerical",
                "values": "numerical values"
            }
        
        # ç±»åˆ«å‹åˆ—
        else:
            unique_values = data[column].unique().tolist()
            unique_values_str = [str(val) for val in unique_values]
            
            data_info["columns"][column] = {
                "type": "categorical",
                "values": unique_values_str,
                "count": len(unique_values)
            }
    
    # ä¿å­˜åˆ°JSONæ–‡ä»¶
    with open(save_path, 'w', encoding='utf-8') as f:
        json.dump(data_info, f, indent=2, ensure_ascii=False)
    
    print(f"åˆ—ä¿¡æ¯å·²ä¿å­˜åˆ°: {save_path}")
    return data_info


def analyze_missing_data(df, missing_values=['unknown', 'Unknown', '?', '', 'NaN', 'N/A', None]):
    """
    ä¸“é—¨åˆ†ææ•°æ®é›†ä¸­çš„ç¼ºå¤±å€¼æƒ…å†µ
    
    Args:
        df: pandas DataFrame
        missing_values: å¸¸è§çš„ç¼ºå¤±å€¼æ ‡è®°åˆ—è¡¨
        
    Returns:
        missing_report: ç¼ºå¤±å€¼ç»Ÿè®¡æŠ¥å‘Šï¼ˆå­—å…¸ï¼‰
    """
    
    missing_report = {
        "dataset_shape": {
            "rows": int(len(df)),
            "columns": int(len(df.columns))
        },
        "missing_analysis": {},
        "recommendations": []
    }
    
    total_rows = len(df)
    columns_with_missing = []
    
    for col in df.columns:
        col_analysis = {
            "column_name": col,
            "dtype": str(df[col].dtype),  # ç¡®ä¿æ˜¯å­—ç¬¦ä¸²ç±»å‹
            "is_categorical": str(df[col].dtype) == 'object',  # è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ¯”è¾ƒ
            "total_missing": 0,
            "missing_percentage": 0.0,
            "missing_types": {}
        }
        
        # 1. ç»Ÿè®¡NaNå€¼
        nan_count = int(df[col].isna().sum())  # è½¬ä¸ºæ•´æ•°
        col_analysis["missing_types"]["NaN"] = {
            "count": nan_count,
            "percentage": round(float(nan_count) / float(total_rows) * 100, 2)
        }
        col_analysis["total_missing"] += nan_count
        
        # 2. ç»Ÿè®¡ç‰¹æ®Šæ ‡è®°çš„ç¼ºå¤±å€¼ï¼ˆå¦‚'unknown'ç­‰ï¼‰
        if col_analysis["is_categorical"]:
            for missing_val in missing_values:
                if missing_val is not None and str(missing_val) in df[col].astype(str).values:
                    # å®‰å…¨åœ°ç»Ÿè®¡ç¼ºå¤±å€¼
                    try:
                        count = int((df[col] == missing_val).sum())
                    except:
                        # å¦‚æœæ˜¯Noneï¼Œä½¿ç”¨isna()
                        count = int(df[col].isna().sum())
                    
                    col_analysis["missing_types"][str(missing_val)] = {
                        "count": count,
                        "percentage": round(float(count) / float(total_rows) * 100, 2)
                    }
                    col_analysis["total_missing"] += count
        
        # è®¡ç®—æ€»ç¼ºå¤±æ¯”ä¾‹
        col_analysis["missing_percentage"] = round(float(col_analysis["total_missing"]) / float(total_rows) * 100, 2)
        col_analysis["total_missing"] = int(col_analysis["total_missing"])  # ç¡®ä¿æ˜¯æ•´æ•°
        
        if col_analysis["total_missing"] > 0:
            columns_with_missing.append(col)
            
            # æ·»åŠ å¤„ç†å»ºè®®
            missing_pct = col_analysis["missing_percentage"]
            if missing_pct < 1:
                suggestion = f"{col}: åˆ é™¤ç¼ºå¤±è¡Œï¼ˆç¼ºå¤±ç‡<1%ï¼‰"
            elif missing_pct < 10:
                suggestion = f"{col}: ç”¨ä¼—æ•°/å¹³å‡æ•°å¡«å……"
            else:
                suggestion = f"{col}: éœ€è¦å»ºæ¨¡å¡«å……æˆ–åˆ é™¤è¯¥åˆ—ï¼ˆç¼ºå¤±ç‡â‰¥10%ï¼‰"
            
            missing_report["recommendations"].append(suggestion)
        
        missing_report["missing_analysis"][col] = col_analysis
    
    # ä¿å­˜æŠ¥å‘Š
    output_file = './dataset/missing_value_analysis.json'
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(missing_report, f, indent=2, ensure_ascii=False)
    
    print("=" * 50)
    print(f"ç¼ºå¤±å€¼åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")
    print(f"åŒ…å«ç¼ºå¤±å€¼çš„åˆ—æ•°: {len(columns_with_missing)}/{len(df.columns)}")
    
    # æ˜¾ç¤ºå…³é”®ä¿¡æ¯
    if columns_with_missing:
        print("\nå…³é”®ç¼ºå¤±ä¿¡æ¯:")
        for col in columns_with_missing:
            info = missing_report["missing_analysis"][col]
            print(f"  {col}: {info['total_missing']}ä¸ªç¼ºå¤± ({info['missing_percentage']}%)")
    else:
        print("\næ— ç¼ºå¤±å€¼!")
    
    return missing_report

def handle_missing_values(data):
    """
    å¤„ç†ç¼ºå¤±å€¼ï¼ˆåŸºäºmissing_value_analysis.jsonçš„å»ºè®®ï¼‰
    
    å‚æ•°:
    data: è¾“å…¥çš„DataFrame
    
    è¿”å›:
    data: å¤„ç†åçš„DataFrame
    """
    
    # job: åˆ é™¤ç¼ºå¤±è¡Œ
    print("å¤„ç† job åˆ— (åˆ é™¤ 'unknown' å€¼)...")
    original_rows = len(data)
    data = data[data['job'] != 'unknown'].copy()
    removed_job_rows = original_rows - len(data)
    print(f"  åˆ é™¤äº† {removed_job_rows} è¡Œ 'unknown' å€¼")
    
    # education: ç”¨ä¼—æ•°å¡«å……
    print("å¤„ç† education åˆ— (ç”¨ä¼—æ•°å¡«å…… 'unknown')...")
    # è®¡ç®—ä¼—æ•°ï¼ˆæ’é™¤unknownï¼‰
    education_mode = data[data['education'] != 'unknown']['education'].mode()
    if len(education_mode) > 0:
        edu_fill_value = education_mode[0]
        edu_missing_count = (data['education'] == 'unknown').sum()
        data['education'] = data['education'].replace('unknown', edu_fill_value)
        print(f"  å°† {edu_missing_count} ä¸ª 'unknown' æ›¿æ¢ä¸º '{edu_fill_value}'")
    else:
        print("  è­¦å‘Šï¼šæ— æ³•æ‰¾åˆ°åˆé€‚çš„å¡«å……å€¼")
    
    # contact: ä¿ç•™ 'unknown' ä¸ºå•ç‹¬ç±»åˆ«
    print("å¤„ç† contact åˆ— (ä¿ç•™ 'unknown' ä¸ºå•ç‹¬ç±»åˆ«)...")
    contact_missing = (data['contact'] == 'unknown').sum()
    print(f"  ä¿ç•™ {contact_missing} ä¸ª 'unknown' ä½œä¸ºåˆ†ç±»å€¼")
    
    # poutcome: åˆ é™¤è¯¥åˆ—
    print("å¤„ç† poutcome åˆ— (åˆ é™¤æ•´åˆ—)...")
    if 'poutcome' in data.columns:
        data = data.drop('poutcome', axis=1)
        print(f"  å·²åˆ é™¤ poutcome åˆ—")
    else:
        print("  è¯¥åˆ—ä¸å­˜åœ¨")
    
    # æ˜¾ç¤ºå¤„ç†ç»“æœ
    print(f"\nå¤„ç†å‰: {original_rows} è¡Œ, 17 åˆ—")
    print(f"å¤„ç†å: {len(data)} è¡Œ, {len(data.columns)} åˆ—")
    print(f"åˆ é™¤äº† {original_rows - len(data)} è¡Œæ•°æ®")
    
    return data

def simple_label_encoding(data):
    """
    ç®€å•æ ‡ç­¾ç¼–ç ï¼Œç›´æ¥è½¬æ¢æ‰€æœ‰ç±»åˆ«åˆ—
    ä¿å­˜ç¼–ç å™¨å’Œå¤„ç†åçš„æ•°æ®åˆ°datasetæ–‡ä»¶å¤¹
    """
    import pickle
    import os
    
    categorical_cols = ['job', 'marital', 'education', 'default', 
                       'housing', 'loan', 'contact', 'month']
    
    # åˆ›å»ºç¼–ç å™¨å­—å…¸
    label_encoders = {}
    
    print("å¼€å§‹æ ‡ç­¾ç¼–ç ...")
    for col in categorical_cols:
        # ç¡®ä¿æ•°æ®ä¸ºå­—ç¬¦ä¸²ç±»å‹
        data[col] = data[col].astype(str)
        
        le = LabelEncoder()
        data[col] = le.fit_transform(data[col])
        label_encoders[col] = le
        
        print(f"  {col}: ç¼–ç å®Œæˆ ({len(le.classes_)}ä¸ªç±»åˆ«)")
    
    # ==================== ä¿å­˜åˆ°datasetæ–‡ä»¶å¤¹ ====================
    dataset_dir = './dataset'
    if not os.path.exists(dataset_dir):
        os.makedirs(dataset_dir)
    
    # 1. ä¿å­˜ç¼–ç åçš„æ•°æ®ä¸ºCSV
    processed_data_path = os.path.join(dataset_dir, 'data_label_encoded.csv')
    data.to_csv(processed_data_path, index=False)
    print(f"\nâœ… ç¼–ç åçš„æ•°æ®å·²ä¿å­˜åˆ°: {processed_data_path}")
    
    # 2. ä¿å­˜ç¼–ç å™¨å¯¹è±¡ä¾›åç»­ä½¿ç”¨
    encoders_path = os.path.join(dataset_dir, 'label_encoders.pkl')
    with open(encoders_path, 'wb') as f:
        pickle.dump(label_encoders, f)
    print(f"âœ… æ ‡ç­¾ç¼–ç å™¨å·²ä¿å­˜åˆ°: {encoders_path}")
    
    # 3. åˆ›å»ºç®€å•çš„æ˜ å°„ä¿¡æ¯æ–‡ä»¶ï¼ˆå¯é€‰ï¼Œç”¨äºæŸ¥çœ‹ï¼‰
    mapping_info = {}
    for col, le in label_encoders.items():
        mapping_info[col] = {
            'classes': le.classes_.tolist(),
            'indices': list(range(len(le.classes_)))
        }
    
    mapping_path = os.path.join(dataset_dir, 'encoding_mapping.json')
    with open(mapping_path, 'w', encoding='utf-8') as f:
        json.dump(mapping_info, f, indent=2, ensure_ascii=False)
    print(f"âœ… ç¼–ç æ˜ å°„ä¿¡æ¯å·²ä¿å­˜åˆ°: {mapping_path}")
    
    # æ˜¾ç¤ºå‰å‡ è¡Œçš„ç¼–ç ç¤ºä¾‹
    print("\nğŸ“Š ç¼–ç ç¤ºä¾‹ (å‰3è¡Œ):")
    sample_cols = ['job', 'education', 'contact']
    for col in sample_cols:
        print(f"  {col} åŸå§‹å€¼ -> ç¼–ç å€¼:")
        for i in range(3):
            original = label_encoders[col].inverse_transform([data[col].iloc[i]])[0]
            encoded = data[col].iloc[i]
            print(f"    ç¬¬{i+1}è¡Œ: '{original}' -> {encoded}")
    
    return data, label_encoders


def load_processed_data():
    """
    ä»datasetæ–‡ä»¶å¤¹åŠ è½½å¤„ç†å¥½çš„æ•°æ®
    """
    import pickle
    
    dataset_dir = './dataset'
    
    # åŠ è½½ç¼–ç åçš„æ•°æ®
    data_path = os.path.join(dataset_dir, 'data_label_encoded.csv')
    if not os.path.exists(data_path):
        raise FileNotFoundError(f"æœªæ‰¾åˆ°å¤„ç†åçš„æ•°æ®æ–‡ä»¶: {data_path}")
    
    data = pd.read_csv(data_path)
    print(f"åŠ è½½å¤„ç†åçš„æ•°æ®: {data.shape[0]}è¡Œ Ã— {data.shape[1]}åˆ—")
    
    # åŠ è½½ç¼–ç å™¨
    encoders_path = os.path.join(dataset_dir, 'label_encoders.pkl')
    if os.path.exists(encoders_path):
        with open(encoders_path, 'rb') as f:
            label_encoders = pickle.load(f)
        print(f"åŠ è½½æ ‡ç­¾ç¼–ç å™¨: {len(label_encoders)}ä¸ªç±»åˆ«åˆ—")
    else:
        label_encoders = None
        print("è­¦å‘Š: æœªæ‰¾åˆ°æ ‡ç­¾ç¼–ç å™¨æ–‡ä»¶")
    
    return data, label_encoders


def decode_columns(data, label_encoders):
    """
    å°†ç¼–ç åçš„åˆ—è§£ç å›åŸå§‹ç±»åˆ«ï¼ˆä»…ç”¨äºæŸ¥çœ‹ï¼‰
    """
    if label_encoders is None:
        print("æ— æ³•è§£ç : æœªæ‰¾åˆ°ç¼–ç å™¨")
        return data
    
    categorical_cols = ['job', 'marital', 'education', 'default', 
                       'housing', 'loan', 'contact', 'month']
    
    decoded_data = data.copy()
    
    for col in categorical_cols:
        if col in label_encoders:
            decoded_data[f'{col}_decoded'] = label_encoders[col].inverse_transform(data[col])
    
    return decoded_data


def main():
    """
    ä¸»å‡½æ•°ï¼šæ‰§è¡Œå®Œæ•´æ•°æ®å¤„ç†æµç¨‹
    """
    # æ­¥éª¤1ï¼šåŠ è½½æ•°æ®
    print("æ­¥éª¤1: åŠ è½½æ•°æ®...")
    data_path = './dataset/bank_marketing.csv'
    data = load_and_rename_data(data_path)
    
    # æ­¥éª¤2ï¼šæ•°æ®æ¢ç´¢
    print("\næ­¥éª¤2: æ•°æ®æ¢ç´¢...")
    explore_data(data)
    
    # æ­¥éª¤3ï¼šç”Ÿæˆåˆ—ä¿¡æ¯
    print("\næ­¥éª¤3: ç”Ÿæˆåˆ—ä¿¡æ¯...")
    generate_column_info(data)
    
    # æ­¥éª¤4ï¼šç¼ºå¤±å€¼åˆ†æ
    print("\næ­¥éª¤4: ç¼ºå¤±å€¼å¤„ç†...")
    analyze_missing_data(data)
    
    # æ­¥éª¤5ï¼šç¼ºå¤±å€¼å¤„ç†
    print("\næ­¥éª¤5: ç¼ºå¤±å€¼å¤„ç†...")
    data = handle_missing_values(data)
    generate_column_info(data, save_path="./dataset/column_info_aftermissing.json")
    
    # æ­¥éª¤6ï¼šæ ‡ç­¾ç¼–ç 
    print("\næ­¥éª¤6: æ ‡ç­¾ç¼–ç ...")
    data, label_encoders = simple_label_encoding(data)  # ç°åœ¨ä¼šä¿å­˜åˆ°datasetæ–‡ä»¶å¤¹
    
    print("\nğŸ‰ æ•°æ®å¤„ç†ç®¡é“å®Œæˆï¼")
    

    


    

def run_full_pipeline(include_download=False):
    """
    è¿è¡Œå®Œæ•´æ•°æ®å¤„ç†ç®¡é“
    
    å‚æ•°:
    include_download: æ˜¯å¦åŒ…å«ä¸‹è½½æ•°æ®é›†æ­¥éª¤
    """
    if include_download:
        print("ä¸‹è½½æ•°æ®é›†...")
        download_dataset()
    
    # è¿è¡Œä¸»æµç¨‹
    return main()

if __name__ == "__main__":
    # è¿è¡Œå®Œæ•´ç®¡é“ï¼ˆä¸åŒ…å«ä¸‹è½½ï¼Œå‡è®¾æ•°æ®å·²å­˜åœ¨ï¼‰
    processed_data = run_full_pipeline(include_download=False)
    print("\næ•°æ®å¤„ç†å®Œæˆ!")
