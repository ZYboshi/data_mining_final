# naive_bayes_model.py
"""
æœ´ç´ è´å¶æ–¯æ¨¡å‹å»ºæ¨¡
ä½¿ç”¨åæŠ˜äº¤å‰éªŒè¯
"""
import pandas as pd
import numpy as np
import os
import time
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB
from sklearn.metrics import (classification_report, confusion_matrix, 
                           roc_auc_score, accuracy_score, precision_score, 
                           recall_score, f1_score, roc_curve, precision_recall_curve)
from sklearn.preprocessing import StandardScaler, MinMaxScaler
import matplotlib.pyplot as plt
import seaborn as sns

# è®¾ç½®matplotlib
try:
    import matplotlib
    matplotlib.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
    matplotlib.rcParams['axes.unicode_minus'] = False
except:
    pass

def create_directories():
    """åˆ›å»ºå¿…è¦çš„æ–‡ä»¶å¤¹"""
    directories = [
        './result/naive_bayes',
        './model_checkpoint/naive_bayes'
    ]
    
    for directory in directories:
        if not os.path.exists(directory):
            os.makedirs(directory)
            print(f"âœ… åˆ›å»ºç›®å½•: {directory}")

def load_and_split_data():
    """åŠ è½½æ•°æ®"""
    print("ğŸ“‚ åŠ è½½æ•°æ®...")
    data = pd.read_csv('./dataset/data_label_encoded.csv')
    
    X = data.drop('deposit', axis=1)
    y = data['deposit']
    
    print(f"æ•°æ®é›†å½¢çŠ¶: {data.shape}")
    print(f"ç‰¹å¾æ•°é‡: {X.shape[1]}")
    print(f"ç±»åˆ«åˆ†å¸ƒ:\n{y.value_counts()}")
    print(f"æ­£ç±»æ¯”ä¾‹ (è®¢é˜…å®šæœŸå­˜æ¬¾): {y.mean():.2%}")
    
    return X, y

def preprocess_for_naive_bayes(X):
    """
    ä¸ºæœ´ç´ è´å¶æ–¯é¢„å¤„ç†æ•°æ®
    """
    print("\nâš™ï¸ æ•°æ®é¢„å¤„ç†...")
    
    # 1. æ•°å€¼ç‰¹å¾æ ‡å‡†åŒ–ï¼ˆé«˜æ–¯æœ´ç´ è´å¶æ–¯éœ€è¦ï¼‰
    numerical_cols = ['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous']
    existing_numerical = [col for col in numerical_cols if col in X.columns]
    
    if len(existing_numerical) > 0:
        print(f"æ•°å€¼ç‰¹å¾ (æ ‡å‡†åŒ–): {existing_numerical}")
        scaler = StandardScaler()
        X_scaled = X.copy()
        X_scaled[existing_numerical] = scaler.fit_transform(X[existing_numerical])
    else:
        X_scaled = X.copy()
    
    return X_scaled

def train_naive_bayes_model(X, y, bayes_type='gaussian'):
    """
    è®­ç»ƒæœ´ç´ è´å¶æ–¯æ¨¡å‹
    bayes_type: 'gaussian', 'bernoulli', 'multinomial'
    """
    print(f"\nğŸ”® è®­ç»ƒ{bayes_type}æœ´ç´ è´å¶æ–¯æ¨¡å‹...")
    
    if bayes_type == 'gaussian':
        model = GaussianNB(var_smoothing=1e-9)
    elif bayes_type == 'bernoulli':
        model = BernoulliNB()
    elif bayes_type == 'multinomial':
        model = MultinomialNB()
    else:
        model = GaussianNB()
    
    return model

def cross_validation_analysis(model, X, y, bayes_type='gaussian'):
    """åæŠ˜äº¤å‰éªŒè¯åˆ†æ"""
    print("\nğŸ“Š åæŠ˜äº¤å‰éªŒè¯åˆ†æ...")
    
    # è½¬æ¢å›pandas DataFrameä»¥ç¡®ä¿æ­£ç¡®ç´¢å¼•
    if isinstance(X, pd.DataFrame):
        X_df = X
    else:
        X_df = pd.DataFrame(X)
    
    # åˆ›å»ºåˆ†å±‚10æŠ˜äº¤å‰éªŒè¯
    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
    
    cv_metrics = {
        'fold': [],
        'accuracy': [],
        'precision': [],
        'recall': [],
        'f1': [],
        'auc': [],
        'train_time': [],
        'pred_time': []
    }
    
    y_all_true = []
    y_all_pred = []
    y_all_proba = []
    
    fold_num = 1
    total_start_time = time.time()
    
    for train_idx, val_idx in cv.split(X_df, y):
        fold_start_time = time.time()
        
        # æ­£ç¡®ä½¿ç”¨iloc
        X_train = X_df.iloc[train_idx]
        X_val = X_df.iloc[val_idx]
        y_train = y.iloc[train_idx]
        y_val = y.iloc[val_idx]
        
        # è®­ç»ƒ
        model.fit(X_train, y_train)
        train_time = time.time() - fold_start_time
        
        # é¢„æµ‹
        pred_start_time = time.time()
        y_pred = model.predict(X_val)
        y_proba = model.predict_proba(X_val)[:, 1]
        pred_time = time.time() - pred_start_time
        
        # è®¡ç®—æŒ‡æ ‡
        cv_metrics['fold'].append(fold_num)
        cv_metrics['accuracy'].append(accuracy_score(y_val, y_pred))
        cv_metrics['precision'].append(precision_score(y_val, y_pred, zero_division=0))
        cv_metrics['recall'].append(recall_score(y_val, y_pred, zero_division=0))
        cv_metrics['f1'].append(f1_score(y_val, y_pred, zero_division=0))
        cv_metrics['auc'].append(roc_auc_score(y_val, y_proba))
        cv_metrics['train_time'].append(train_time)
        cv_metrics['pred_time'].append(pred_time)
        
        # æ”¶é›†æ‰€æœ‰é¢„æµ‹
        y_all_true.extend(y_val)
        y_all_pred.extend(y_pred)
        y_all_proba.extend(y_proba)
        
        print(f"  ç¬¬{fold_num}æŠ˜: å‡†ç¡®ç‡={accuracy_score(y_val, y_pred):.4f}, "
              f"AUC={roc_auc_score(y_val, y_proba):.4f}")
        
        fold_num += 1
    
    total_time = time.time() - total_start_time
    
    # åˆ›å»ºæŒ‡æ ‡DataFrame
    metrics_df = pd.DataFrame(cv_metrics)
    
    print("\n" + "="*50)
    print(f"ğŸ“ˆ {bayes_type}æœ´ç´ è´å¶æ–¯äº¤å‰éªŒè¯ç»“æœæ±‡æ€»:")
    print("="*50)
    
    print("\nå„æŠ˜è¯¦ç»†æŒ‡æ ‡:")
    print(metrics_df[['fold', 'accuracy', 'precision', 'recall', 'f1', 'auc']].round(4))
    
    print(f"\nğŸ† å¹³å‡æŒ‡æ ‡ (Â±æ ‡å‡†å·®):")
    for metric in ['accuracy', 'precision', 'recall', 'f1', 'auc']:
        mean_val = metrics_df[metric].mean()
        std_val = metrics_df[metric].std()
        print(f"  {metric}: {mean_val:.4f} Â± {std_val:.4f}")
    
    print(f"\nâ±ï¸  æ—¶é—´ç»Ÿè®¡:")
    print(f"  å¹³å‡æ¯æŠ˜è®­ç»ƒæ—¶é—´: {metrics_df['train_time'].mean():.4f}ç§’")
    print(f"  å¹³å‡æ¯æŠ˜é¢„æµ‹æ—¶é—´: {metrics_df['pred_time'].mean():.4f}ç§’")
    print(f"  æ€»è¿è¡Œæ—¶é—´: {total_time:.2f}ç§’")
    
    return metrics_df, np.array(y_all_true), np.array(y_all_pred), np.array(y_all_proba), cv

def analyze_model_probabilities(model, X_df, y, bayes_type='gaussian'):
    """
    åˆ†ææ¨¡å‹çš„æ¦‚ç‡åˆ†å¸ƒ
    """
    print(f"\nğŸ” åˆ†æ{bayes_type}æœ´ç´ è´å¶æ–¯çš„æ¦‚ç‡åˆ†å¸ƒ...")
    
    # é¢„æµ‹æ¦‚ç‡
    model.fit(X_df, y)
    y_proba = model.predict_proba(X_df)
    
    # å¯è§†åŒ–æ¦‚ç‡åˆ†å¸ƒ
    fig, axes = plt.subplots(1, 2, figsize=(15, 6))
    
    # 1. æ¦‚ç‡åˆ†å¸ƒç›´æ–¹å›¾
    ax1 = axes[0]
    ax1.hist(y_proba[:, 1], bins=50, alpha=0.7, color='skyblue', edgecolor='black')
    ax1.set_xlabel('é¢„æµ‹æ¦‚ç‡ (è®¢é˜…å®šæœŸå­˜æ¬¾)')
    ax1.set_ylabel('é¢‘æ•°')
    ax1.set_title(f'é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ - {bayes_type}æœ´ç´ è´å¶æ–¯')
    ax1.grid(True, alpha=0.3)
    
    # 2. æŒ‰çœŸå®ç±»åˆ«åˆ†ç»„çš„æ¦‚ç‡åˆ†å¸ƒ
    ax2 = axes[1]
    colors = ['red', 'green']
    labels = ['æœªè®¢é˜…', 'å·²è®¢é˜…']
    
    for i in [0, 1]:
        mask = (y == i)
        ax2.hist(y_proba[mask, 1], bins=30, alpha=0.6, 
                color=colors[i], label=labels[i])
    
    ax2.set_xlabel('é¢„æµ‹æ¦‚ç‡')
    ax2.set_ylabel('é¢‘æ•°')
    ax2.set_title(f'æŒ‰çœŸå®ç±»åˆ«åˆ†ç»„çš„æ¦‚ç‡åˆ†å¸ƒ')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    save_path = f'./result/naive_bayes/probability_distribution_{bayes_type}.png'
    plt.savefig(save_path, dpi=100, bbox_inches='tight')
    print(f"âœ… æ¦‚ç‡åˆ†å¸ƒå›¾å·²ä¿å­˜åˆ°: {save_path}")
    plt.show()
    
    # è¾“å‡ºæ¨¡å‹å‚æ•°
    print(f"\nğŸ“Š {bayes_type}æœ´ç´ è´å¶æ–¯å‚æ•°:")
    if hasattr(model, 'class_prior_'):
        print(f"ç±»å…ˆéªŒæ¦‚ç‡: {model.class_prior_}")
    
    if bayes_type == 'gaussian' and hasattr(model, 'theta_'):
        print(f"\nå‰5ä¸ªç‰¹å¾çš„ç±»æ¡ä»¶å‡å€¼:")
        for i in range(min(5, len(model.theta_[0]))):
            print(f"  ç‰¹å¾ {i}: ç±»0={model.theta_[0][i]:.4f}, ç±»1={model.theta_[1][i]:.4f}")

def evaluate_final_model(model, X_df, y, bayes_type='gaussian'):
    """è¯„ä¼°æœ€ç»ˆæ¨¡å‹"""
    print(f"\nğŸ¯ è¯„ä¼°{bayes_type}æœ´ç´ è´å¶æ–¯æœ€ç»ˆæ¨¡å‹...")
    
    # åˆ†å‰²æ•°æ®é›†
    X_train, X_test, y_train, y_test = train_test_split(
        X_df, y, test_size=0.2, random_state=42, stratify=y
    )
    
    # è®­ç»ƒæœ€ç»ˆæ¨¡å‹
    start_time = time.time()
    model.fit(X_train, y_train)
    train_time = time.time() - start_time
    
    print(f"æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {train_time:.4f}ç§’")
    
    # é¢„æµ‹
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)[:, 1]
    
    # è¯„ä¼°
    print("\nğŸ“‹ æµ‹è¯•é›†è¯„ä¼°ç»“æœ:")
    print(classification_report(y_test, y_pred))
    
    auc_score = roc_auc_score(y_test, y_proba)
    print(f"AUC Score: {auc_score:.4f}")
    
    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y_test, y_pred)
    print(f"\næ··æ·†çŸ©é˜µ:\n{cm}")
    
    # å¯è§†åŒ–æ··æ·†çŸ©é˜µ
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=['No Deposit', 'Deposit'],
                yticklabels=['No Deposit', 'Deposit'])
    plt.title(f'{bayes_type.capitalize()} Naive Bayes - Confusion Matrix')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.tight_layout()
    
    save_path = f'./result/naive_bayes/confusion_matrix_{bayes_type}.png'
    plt.savefig(save_path, dpi=100, bbox_inches='tight')
    print(f"âœ… æ··æ·†çŸ©é˜µå›¾å·²ä¿å­˜åˆ°: {save_path}")
    plt.show()
    
    # ROCæ›²çº¿
    fpr, tpr, thresholds = roc_curve(y_test, y_proba)
    
    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, 'b-', label=f'{bayes_type.capitalize()} NB (AUC = {auc_score:.3f})')
    plt.plot([0, 1], [0, 1], 'r--', label='Random Classifier')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f'ROC Curve - {bayes_type.capitalize()} Naive Bayes')
    plt.legend(loc='lower right')
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    
    save_path = f'./result/naive_bayes/roc_curve_{bayes_type}.png'
    plt.savefig(save_path, dpi=100, bbox_inches='tight')
    print(f"âœ… ROCæ›²çº¿å›¾å·²ä¿å­˜åˆ°: {save_path}")
    plt.show()
    
    # Precision-Recallæ›²çº¿
    precision, recall, thresholds_pr = precision_recall_curve(y_test, y_proba)
    
    plt.figure(figsize=(8, 6))
    plt.plot(recall, precision, 'g-', linewidth=2)
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title(f'Precision-Recall Curve - {bayes_type.capitalize()} NB')
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    
    save_path = f'./result/naive_bayes/precision_recall_curve_{bayes_type}.png'
    plt.savefig(save_path, dpi=100, bbox_inches='tight')
    print(f"âœ… Precision-Recallæ›²çº¿å›¾å·²ä¿å­˜åˆ°: {save_path}")
    plt.show()
    
    return model, X_test, y_test, y_pred, y_proba

def compare_different_nb_types_simple(X_df, y):
    """ç®€å•æ¯”è¾ƒä¸åŒç±»å‹çš„æœ´ç´ è´å¶æ–¯"""
    print("\nâš–ï¸ æ¯”è¾ƒä¸åŒç±»å‹çš„æœ´ç´ è´å¶æ–¯...")
    
    nb_types = ['gaussian', 'bernoulli', 'multinomial']
    results = {}
    
    for nb_type in nb_types:
        print(f"\næµ‹è¯• {nb_type} æœ´ç´ è´å¶æ–¯...")
        
        # å¤åˆ¶æ•°æ®é¿å…ä¿®æ”¹åŸå§‹æ•°æ®
        X_temp = X_df.copy()
        
        if nb_type == 'multinomial':
            # å¤šé¡¹æœ´ç´ è´å¶æ–¯éœ€è¦éè´Ÿç‰¹å¾
            scaler = MinMaxScaler()
            X_temp = pd.DataFrame(scaler.fit_transform(X_temp), 
                                 columns=X_temp.columns)
        
        # åˆ›å»ºæ¨¡å‹
        model = train_naive_bayes_model(X_temp, y, nb_type)
        
        # ä½¿ç”¨5æŠ˜äº¤å‰éªŒè¯å¿«é€Ÿè¯„ä¼°
        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
        auc_scores = []
        accuracy_scores = []
        
        for train_idx, val_idx in cv.split(X_temp, y):
            X_train = X_temp.iloc[train_idx]
            X_val = X_temp.iloc[val_idx]
            y_train = y.iloc[train_idx]
            y_val = y.iloc[val_idx]
            
            model.fit(X_train, y_train)
            y_proba = model.predict_proba(X_val)[:, 1]
            y_pred = model.predict(X_val)
            
            auc_scores.append(roc_auc_score(y_val, y_proba))
            accuracy_scores.append(accuracy_score(y_val, y_pred))
        
        results[nb_type] = {
            'mean_auc': np.mean(auc_scores),
            'std_auc': np.std(auc_scores),
            'mean_accuracy': np.mean(accuracy_scores),
            'std_accuracy': np.std(accuracy_scores)
        }
        
        print(f"  AUC: {np.mean(auc_scores):.4f} Â± {np.std(auc_scores):.4f}")
        print(f"  å‡†ç¡®ç‡: {np.mean(accuracy_scores):.4f} Â± {np.std(accuracy_scores):.4f}")
    
    # å¯è§†åŒ–æ¯”è¾ƒ
    fig, axes = plt.subplots(1, 2, figsize=(15, 6))
    
    # AUCæ¯”è¾ƒ
    ax1 = axes[0]
    types = list(results.keys())
    auc_means = [results[t]['mean_auc'] for t in types]
    auc_stds = [results[t]['std_auc'] for t in types]
    
    bars1 = ax1.bar(types, auc_means, yerr=auc_stds, capsize=10, 
                   color=['blue', 'green', 'orange'], alpha=0.7)
    ax1.axhline(y=0.5, color='red', linestyle='--', alpha=0.5, label='Random (AUC=0.5)')
    ax1.set_xlabel('Naive Bayes Type')
    ax1.set_ylabel('AUC Score')
    ax1.set_title('AUC Comparison')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # å‡†ç¡®ç‡æ¯”è¾ƒ
    ax2 = axes[1]
    acc_means = [results[t]['mean_accuracy'] for t in types]
    acc_stds = [results[t]['std_accuracy'] for t in types]
    
    bars2 = ax2.bar(types, acc_means, yerr=acc_stds, capsize=10, 
                   color=['cyan', 'lime', 'gold'], alpha=0.7)
    ax2.set_xlabel('Naive Bayes Type')
    ax2.set_ylabel('Accuracy')
    ax2.set_title('Accuracy Comparison')
    ax2.grid(True, alpha=0.3)
    
    # åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼
    for bars, ax, means in zip([bars1, bars2], [ax1, ax2], [auc_means, acc_means]):
        for bar, mean_val in zip(bars, means):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                   f'{mean_val:.3f}', ha='center', va='bottom', fontsize=9)
    
    plt.tight_layout()
    save_path = './result/naive_bayes/nb_type_comparison.png'
    plt.savefig(save_path, dpi=100, bbox_inches='tight')
    print(f"âœ… æœ´ç´ è´å¶æ–¯ç±»å‹æ¯”è¾ƒå›¾å·²ä¿å­˜åˆ°: {save_path}")
    plt.show()
    
    # ç¡®å®šæœ€ä½³ç±»å‹
    best_type_auc = max(results.items(), key=lambda x: x[1]['mean_auc'])
    best_type_acc = max(results.items(), key=lambda x: x[1]['mean_accuracy'])
    
    print(f"\nğŸŒŸ åŸºäºAUCçš„æœ€ä½³ç±»å‹: {best_type_auc[0]} (AUC = {best_type_auc[1]['mean_auc']:.4f})")
    print(f"ğŸŒŸ åŸºäºå‡†ç¡®ç‡çš„æœ€ä½³ç±»å‹: {best_type_acc[0]} (å‡†ç¡®ç‡ = {best_type_acc[1]['mean_accuracy']:.4f})")
    
    # é€‰æ‹©AUCæœ€ä½³çš„ç±»å‹
    return best_type_auc[0], results

def save_results(model, metrics_df, bayes_type='gaussian'):
    """ä¿å­˜æ¨¡å‹å’Œç»“æœ"""
    print(f"\nğŸ’¾ ä¿å­˜{bayes_type}æœ´ç´ è´å¶æ–¯ç»“æœ...")
    
    import pickle
    import json
    
    # ä¿å­˜æ¨¡å‹
    model_path = f'./model_checkpoint/naive_bayes/{bayes_type}_naive_bayes_model.pkl'
    with open(model_path, 'wb') as f:
        pickle.dump(model, f)
    print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}")
    
    # ä¿å­˜æŒ‡æ ‡
    metrics_path = f'./result/naive_bayes/{bayes_type}_cross_validation_metrics.csv'
    metrics_df.to_csv(metrics_path, index=False)
    print(f"âœ… äº¤å‰éªŒè¯æŒ‡æ ‡å·²ä¿å­˜åˆ°: {metrics_path}")
    
    # ä¿å­˜æ¨¡å‹å‚æ•°
    params_path = f'./result/naive_bayes/{bayes_type}_model_params.json'
    params_dict = model.get_params()
    
    if hasattr(model, 'class_prior_'):
        params_dict['class_prior'] = model.class_prior_.tolist()
    if hasattr(model, 'theta_'):
        params_dict['class_0_means'] = model.theta_[0].tolist()
        params_dict['class_1_means'] = model.theta_[1].tolist()
    
    with open(params_path, 'w') as f:
        json.dump(params_dict, f, indent=2)
    
    print(f"âœ… æ¨¡å‹å‚æ•°å·²ä¿å­˜åˆ°: {params_path}")
    
    # ä¿å­˜æ€§èƒ½æ€»ç»“
    summary_path = f'./result/naive_bayes/{bayes_type}_performance_summary.txt'
    with open(summary_path, 'w') as f:
        f.write(f"{bayes_type.capitalize()} Naive Bayes Performance Summary\n")
        f.write("="*50 + "\n\n")
        f.write(f"Average Accuracy: {metrics_df['accuracy'].mean():.4f} Â± {metrics_df['accuracy'].std():.4f}\n")
        f.write(f"Average Precision: {metrics_df['precision'].mean():.4f} Â± {metrics_df['precision'].std():.4f}\n")
        f.write(f"Average Recall: {metrics_df['recall'].mean():.4f} Â± {metrics_df['recall'].std():.4f}\n")
        f.write(f"Average F1 Score: {metrics_df['f1'].mean():.4f} Â± {metrics_df['f1'].std():.4f}\n")
        f.write(f"Average AUC: {metrics_df['auc'].mean():.4f} Â± {metrics_df['auc'].std():.4f}\n")
        f.write(f"Average Training Time: {metrics_df['train_time'].mean():.4f}ç§’\n")
        f.write(f"Average Prediction Time: {metrics_df['pred_time'].mean():.4f}ç§’\n")
    
    print(f"âœ… æ€§èƒ½æ€»ç»“å·²ä¿å­˜åˆ°: {summary_path}")

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸ”® Naive Bayes Modeling (10-Fold CV)")
    print("=" * 60)
    
    # 0. åˆ›å»ºç›®å½•
    create_directories()
    
    # 1. åŠ è½½æ•°æ®
    X, y = load_and_split_data()
    
    # 2. é¢„å¤„ç†
    X_processed = preprocess_for_naive_bayes(X)
    print(f"å¤„ç†åçš„æ•°æ®å½¢çŠ¶: {X_processed.shape}")
    
    # 3. ç®€å•æ¯”è¾ƒä¸åŒçš„æœ´ç´ è´å¶æ–¯ç±»å‹
    best_type, comparison_results = compare_different_nb_types_simple(X_processed, y)
    
    # 4. ä½¿ç”¨æœ€ä½³ç±»å‹è¿›è¡Œè¯¦ç»†åˆ†æ
    print(f"\n{'='*60}")
    print(f"ğŸ¯ è¯¦ç»†åˆ†ææœ€ä½³ç±»å‹: {best_type}")
    print('='*60)
    
    # æ ¹æ®ä¸åŒç±»å‹å¯èƒ½éœ€è¦ä¸åŒçš„é¢„å¤„ç†
    X_final = X_processed.copy()
    if best_type == 'multinomial':
        # å¤šé¡¹æœ´ç´ è´å¶æ–¯éœ€è¦éè´Ÿç‰¹å¾
        scaler = MinMaxScaler()
        X_final = pd.DataFrame(scaler.fit_transform(X_final), columns=X_final.columns)
    
    # åˆ›å»ºæ¨¡å‹
    model = train_naive_bayes_model(X_final, y, best_type)
    
    # 5. åæŠ˜äº¤å‰éªŒè¯
    metrics_df, y_all_true, y_all_pred, y_all_proba, cv = cross_validation_analysis(
        model, X_final, y, best_type
    )
    
    # 6. åˆ†ææ¦‚ç‡åˆ†å¸ƒ
    analyze_model_probabilities(model, X_final, y, best_type)
    
    # 7. è®­ç»ƒæœ€ç»ˆæ¨¡å‹å¹¶å®Œæ•´è¯„ä¼°
    final_model, X_test, y_test, y_pred, y_proba = evaluate_final_model(
        model, X_final, y, best_type
    )
    
    # 8. ä¿å­˜ç»“æœ
    save_results(final_model, metrics_df, best_type)
    
    print("\n" + "="*60)
    print(f"âœ… {best_type.capitalize()} Naive Bayes Modeling Completed!")
    print("="*60)
    
    print(f"\nğŸ“Š æ€§èƒ½æ€»ç»“:")
    print(f"  å¹³å‡å‡†ç¡®ç‡: {metrics_df['accuracy'].mean():.4f}")
    print(f"  å¹³å‡AUC: {metrics_df['auc'].mean():.4f}")
    print(f"  å¹³å‡F1åˆ†æ•°: {metrics_df['f1'].mean():.4f}")
    print(f"  å¹³å‡å¬å›ç‡: {metrics_df['recall'].mean():.4f}")
    print(f"  å¹³å‡ç²¾ç¡®ç‡: {metrics_df['precision'].mean():.4f}")
    
    print(f"\nâ±ï¸  æ—¶é—´æ•ˆç‡:")
    print(f"  å¹³å‡æ¯æŠ˜è®­ç»ƒæ—¶é—´: {metrics_df['train_time'].mean():.4f}ç§’")
    print(f"  å¹³å‡æ¯æŠ˜é¢„æµ‹æ—¶é—´: {metrics_df['pred_time'].mean():.4f}ç§’")
    
    print(f"\nğŸ“ ç»“æœæ–‡ä»¶ä¿å­˜åœ¨:")
    print(f"  ./result/naive_bayes/ - åŒ…å«æ‰€æœ‰å›¾ç‰‡å’ŒCSVæ–‡ä»¶")
    print(f"  ./model_checkpoint/naive_bayes/ - åŒ…å«è®­ç»ƒå¥½çš„æ¨¡å‹")
    
    return final_model, metrics_df, comparison_results

if __name__ == "__main__":
    try:
        start_time = time.time()
        model, metrics_df, results = main()
        total_time = time.time() - start_time
        
        print(f"\nâ±ï¸  æ€»è¿è¡Œæ—¶é—´: {total_time:.2f}ç§’")
        print("ğŸ‰ æœ´ç´ è´å¶æ–¯å»ºæ¨¡å®Œæˆ!")
        
    except FileNotFoundError as e:
        print(f"\nâŒ æ–‡ä»¶æœªæ‰¾åˆ°: {e}")
        print("è¯·ç¡®ä¿ './dataset/data_label_encoded.csv' æ–‡ä»¶å­˜åœ¨")
        print("å°è¯•è¿è¡Œ: python data_preprocessing.py å…ˆè¿›è¡Œæ•°æ®é¢„å¤„ç†")
    except Exception as e:
        print(f"\nâŒ è¿è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        print("\nğŸ’¡ å¦‚æœé—®é¢˜æŒç»­ï¼Œå¯ä»¥:")
        print("1. æ£€æŸ¥æ•°æ®æ–‡ä»¶è·¯å¾„")
        print("2. ç¡®ä¿æ‰€æœ‰å¿…è¦çš„åŒ…å·²å®‰è£…")
        print("3. å°è¯•é‡å¯Pythonç¯å¢ƒ")
