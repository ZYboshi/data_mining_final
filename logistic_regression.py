# logistic_regression_model.py
"""
é€»è¾‘å›å½’æ¨¡å‹å»ºæ¨¡
ä½¿ç”¨åæŠ˜äº¤å‰éªŒè¯
"""
import pandas as pd
import numpy as np
import os
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (classification_report, confusion_matrix, 
                           roc_auc_score, accuracy_score, precision_score, 
                           recall_score, f1_score)
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns

# è®¾ç½®matplotlibæ”¯æŒä¸­æ–‡æ˜¾ç¤º
try:
    import matplotlib
    matplotlib.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
    matplotlib.rcParams['axes.unicode_minus'] = False
except:
    pass

def create_directories():
    """åˆ›å»ºå¿…è¦çš„æ–‡ä»¶å¤¹"""
    directories = [
        './result/logistic',
        './model_checkpoint/logistic',
        './result',
        './model_checkpoint'
    ]
    
    for directory in directories:
        if not os.path.exists(directory):
            os.makedirs(directory)
            print(f"âœ… åˆ›å»ºç›®å½•: {directory}")

def load_and_split_data():
    """åŠ è½½å¹¶åˆ†å‰²æ•°æ®"""
    print("ğŸ“‚ åŠ è½½æ•°æ®...")
    data = pd.read_csv('./dataset/data_label_encoded.csv')
    
    # åˆ†ç¦»ç‰¹å¾å’Œç›®æ ‡
    X = data.drop('deposit', axis=1)  # ç‰¹å¾
    y = data['deposit']  # ç›®æ ‡å˜é‡
    
    # æŸ¥çœ‹æ•°æ®åŸºæœ¬æƒ…å†µ
    print(f"æ•°æ®é›†å½¢çŠ¶: {data.shape}")
    print(f"ç›®æ ‡å˜é‡åˆ†å¸ƒ:\n{y.value_counts()}")
    print(f"ç±»åˆ«æ¯”ä¾‹ [æ˜¯/å¦]: {y.mean():.2%} / {(1-y.mean()):.2%}")
    
    return X, y

def standardize_features(X):
    """æ ‡å‡†åŒ–æ•°å€¼ç‰¹å¾"""
    print("\nâš™ï¸ æ ‡å‡†åŒ–ç‰¹å¾...")
    
    # æ•°å€¼ç‰¹å¾åˆ—
    numerical_cols = ['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous']
    
    # åªé€‰æ‹©å­˜åœ¨çš„åˆ—
    existing_cols = [col for col in numerical_cols if col in X.columns]
    print(f"æ ‡å‡†åŒ–çš„æ•°å€¼ç‰¹å¾: {existing_cols}")
    
    scaler = StandardScaler()
    X_scaled = X.copy()
    X_scaled[existing_cols] = scaler.fit_transform(X[existing_cols])
    
    return X_scaled

def train_with_cross_validation(X, y):
    """
    ä½¿ç”¨åæŠ˜äº¤å‰éªŒè¯è®­ç»ƒé€»è¾‘å›å½’
    """
    print("\nğŸ¯ å¼€å§‹åæŠ˜äº¤å‰éªŒè¯é€»è¾‘å›å½’...")
    
    # 1. åˆ›å»ºæ¨¡å‹
    model = LogisticRegression(
        max_iter=1000,  # å¢åŠ è¿­ä»£æ¬¡æ•°ç¡®ä¿æ”¶æ•›
        random_state=42,
        C=1.0  # æ­£åˆ™åŒ–å¼ºåº¦ï¼Œé»˜è®¤å€¼
    )
    
    # 2. åˆ›å»ºåˆ†å±‚10æŠ˜äº¤å‰éªŒè¯ï¼ˆä¿æŒç±»åˆ«æ¯”ä¾‹ï¼‰
    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
    
    print("æ­£åœ¨è¿›è¡Œ10æŠ˜äº¤å‰éªŒè¯...")
    
    # 3. è®¡ç®—äº¤å‰éªŒè¯å¾—åˆ†
    cv_scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')
    
    print(f"\nğŸ“Š äº¤å‰éªŒè¯ç»“æœ:")
    print(f"  å¹³å‡å‡†ç¡®ç‡: {cv_scores.mean():.4f}")
    print(f"  å‡†ç¡®ç‡æ ‡å‡†å·®: {cv_scores.std():.4f}")
    print(f"  æ¯æŠ˜å‡†ç¡®ç‡: {cv_scores.round(4)}")
    
    return model, cv, cv_scores

def detailed_cv_analysis(model, X, y, cv):
    """è¯¦ç»†çš„äº¤å‰éªŒè¯åˆ†æ"""
    print("\nğŸ” è¯¦ç»†äº¤å‰éªŒè¯åˆ†æ...")
    
    # æ”¶é›†æ¯æŠ˜çš„é¢„æµ‹ç»“æœ
    cv_metrics = {
        'fold': [],
        'accuracy': [],
        'precision': [],
        'recall': [],
        'f1': [],
        'auc': []
    }
    
    # ç”¨äºå­˜å‚¨æ‰€æœ‰é¢„æµ‹
    y_all_pred = []
    y_all_true = []
    y_all_proba = []
    
    fold_num = 1
    for train_idx, val_idx in cv.split(X, y):
        # åˆ†å‰²æ•°æ®
        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
        
        # è®­ç»ƒæ¨¡å‹
        model.fit(X_train, y_train)
        
        # é¢„æµ‹
        y_pred = model.predict(X_val)
        y_proba = model.predict_proba(X_val)[:, 1]
        
        # è®¡ç®—æŒ‡æ ‡
        cv_metrics['fold'].append(fold_num)
        cv_metrics['accuracy'].append(accuracy_score(y_val, y_pred))
        cv_metrics['precision'].append(precision_score(y_val, y_pred, zero_division=0))
        cv_metrics['recall'].append(recall_score(y_val, y_pred, zero_division=0))
        cv_metrics['f1'].append(f1_score(y_val, y_pred, zero_division=0))
        cv_metrics['auc'].append(roc_auc_score(y_val, y_proba))
        
        # æ”¶é›†æ‰€æœ‰é¢„æµ‹
        y_all_true.extend(y_val)
        y_all_pred.extend(y_pred)
        y_all_proba.extend(y_proba)
        
        fold_num += 1
    
    # åˆ›å»ºæŒ‡æ ‡DataFrame
    metrics_df = pd.DataFrame(cv_metrics)
    
    print("\nğŸ“ˆ æ¯æŠ˜è¯¦ç»†æŒ‡æ ‡:")
    print(metrics_df.round(4))
    
    print("\nğŸŒŸ å¹³å‡æŒ‡æ ‡:")
    print(f"  å‡†ç¡®ç‡: {metrics_df['accuracy'].mean():.4f} Â± {metrics_df['accuracy'].std():.4f}")
    print(f"  ç²¾ç¡®ç‡: {metrics_df['precision'].mean():.4f} Â± {metrics_df['precision'].std():.4f}")
    print(f"  å¬å›ç‡: {metrics_df['recall'].mean():.4f} Â± {metrics_df['recall'].std():.4f}")
    print(f"  F1åˆ†æ•°: {metrics_df['f1'].mean():.4f} Â± {metrics_df['f1'].std():.4f}")
    print(f"  AUC: {metrics_df['auc'].mean():.4f} Â± {metrics_df['auc'].std():.4f}")
    
    return np.array(y_all_true), np.array(y_all_pred), np.array(y_all_proba), metrics_df

def analyze_feature_importance(model, X):
    """åˆ†æç‰¹å¾é‡è¦æ€§"""
    print("\nğŸ”¬ ç‰¹å¾é‡è¦æ€§åˆ†æ:")
    
    feature_names = X.columns
    coefficients = model.coef_[0]
    
    # åˆ›å»ºç‰¹å¾é‡è¦æ€§DataFrame
    importance_df = pd.DataFrame({
        'feature': feature_names,
        'coefficient': coefficients,
        'abs_coefficient': np.abs(coefficients)
    })
    
    # æŒ‰ç»å¯¹ç³»æ•°æ’åº
    importance_df = importance_df.sort_values('abs_coefficient', ascending=False)
    
    print("\nFeature Importance (sorted by impact):")
    print(importance_df[['feature', 'coefficient']].round(4).head(10))
    
    # å¯è§†åŒ– - ä½¿ç”¨è‹±æ–‡é¿å…ä¸­æ–‡å­—ä½“é—®é¢˜
    plt.figure(figsize=(10, 6))
    top_10 = importance_df.head(10)
    colors = ['red' if coef < 0 else 'green' for coef in top_10['coefficient']]
    plt.barh(range(len(top_10)), top_10['abs_coefficient'], color=colors)
    plt.yticks(range(len(top_10)), top_10['feature'])
    plt.xlabel('Coefficient Absolute Value')
    plt.title('Logistic Regression - Top 10 Most Important Features')
    plt.tight_layout()
    
    # ä¿å­˜å›¾ç‰‡ï¼ˆç¡®ä¿æ–‡ä»¶å¤¹å­˜åœ¨ï¼‰
    save_path = './result/logistic/logistic_feature_importance.png'
    plt.savefig(save_path, dpi=100, bbox_inches='tight')
    print(f"âœ… ç‰¹å¾é‡è¦æ€§å›¾å·²ä¿å­˜åˆ°: {save_path}")
    plt.show()
    
    return importance_df

def final_train_and_evaluate(X, y):
    """æœ€ç»ˆè®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹"""
    print("\nğŸ¯ æœ€ç»ˆæ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°...")
    
    # åˆ†å‰²è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    # è®­ç»ƒæœ€ç»ˆæ¨¡å‹
    final_model = LogisticRegression(max_iter=1000, random_state=42)
    final_model.fit(X_train, y_train)
    
    # é¢„æµ‹
    y_pred = final_model.predict(X_test)
    y_proba = final_model.predict_proba(X_test)[:, 1]
    
    # è¯„ä¼°
    print("\nğŸ“‹ æµ‹è¯•é›†è¯„ä¼°ç»“æœ:")
    print(classification_report(y_test, y_pred))
    print(f"AUC Score: {roc_auc_score(y_test, y_proba):.4f}")
    
    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y_test, y_pred)
    print(f"\næ··æ·†çŸ©é˜µ:\n{cm}")
    
    # å¯è§†åŒ–æ··æ·†çŸ©é˜µ - ç”¨ç®€å•è‹±æ–‡
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title('Confusion Matrix')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.tight_layout()
    
    # ä¿å­˜å›¾ç‰‡
    save_path = './result/logistic/logistic_confusion_matrix.png'
    plt.savefig(save_path, dpi=100, bbox_inches='tight')
    print(f"âœ… æ··æ·†çŸ©é˜µå›¾å·²ä¿å­˜åˆ°: {save_path}")
    plt.show()
    
    return final_model

def save_model_and_metrics(model, metrics_df, X_columns):
    """ä¿å­˜æ¨¡å‹å’ŒæŒ‡æ ‡"""
    print("\nğŸ’¾ ä¿å­˜æ¨¡å‹å’ŒæŒ‡æ ‡...")
    
    # 1. ä¿å­˜æ¨¡å‹
    model_path = './model_checkpoint/logistic/logistic_regression_model.pkl'
    import pickle
    with open(model_path, 'wb') as f:
        pickle.dump(model, f)
    print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}")
    
    # 2. ä¿å­˜æŒ‡æ ‡
    metrics_path = './result/logistic/logistic_metrics.csv'
    metrics_df.to_csv(metrics_path, index=False)
    print(f"âœ… æŒ‡æ ‡å·²ä¿å­˜åˆ°: {metrics_path}")
    
    # 3. ä¿å­˜ç‰¹å¾åï¼ˆç”¨äºåç»­é¢„æµ‹ï¼‰
    features_path = './model_checkpoint/logistic/logistic_features.npy'
    np.save(features_path, X_columns)
    print(f"âœ… ç‰¹å¾åå·²ä¿å­˜åˆ°: {features_path}")

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("Logistic Regression Modeling (10-Fold CV)")
    print("=" * 60)
    
    # 0. åˆ›å»ºå¿…è¦çš„æ–‡ä»¶å¤¹
    create_directories()
    
    # 1. åŠ è½½æ•°æ®
    X, y = load_and_split_data()
    
    # 2. æ ‡å‡†åŒ–ç‰¹å¾
    X_scaled = standardize_features(X)
    
    # 3. äº¤å‰éªŒè¯è®­ç»ƒ
    model, cv, cv_scores = train_with_cross_validation(X_scaled, y)
    
    # 4. è¯¦ç»†åˆ†æ
    y_all_true, y_all_pred, y_all_proba, metrics_df = detailed_cv_analysis(
        model, X_scaled, y, cv
    )
    
    # 5. é‡æ–°åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šè®­ç»ƒä»¥åˆ†æç‰¹å¾é‡è¦æ€§
    print("\n" + "=" * 40)
    print("Training final model on full dataset...")
    final_model = LogisticRegression(max_iter=1000, random_state=42)
    final_model.fit(X_scaled, y)
    
    # 6. åˆ†æç‰¹å¾é‡è¦æ€§
    importance_df = analyze_feature_importance(final_model, X_scaled)
    
    # 7. è®­ç»ƒæœ€ç»ˆæ¨¡å‹å¹¶è¯„ä¼°
    final_model = final_train_and_evaluate(X_scaled, y)
    
    # 8. ä¿å­˜æ‰€æœ‰å†…å®¹
    save_model_and_metrics(final_model, metrics_df, X_scaled.columns)
    
    print("\nâœ… Logistic Regression Modeling Completed!")
    print(f"Average Cross-Validation Accuracy: {cv_scores.mean():.4f}")
    print(f"Files saved in ./result/logistic/ and ./model_checkpoint/logistic/")
    
    return final_model, metrics_df, importance_df

if __name__ == "__main__":
    try:
        model, metrics_df, importance_df = main()
        print("\nğŸ‰ æ‰€æœ‰æ“ä½œæˆåŠŸå®Œæˆ!")
        print(f"æŸ¥çœ‹ç»“æœæ–‡ä»¶å¤¹: './result/logistic/'")
    except Exception as e:
        print(f"\nâŒ è¿è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        print("\næ£€æŸ¥ä»¥ä¸‹äº‹é¡¹:")
        print("1. ç¡®ä¿ './dataset/data_label_encoded.csv' æ–‡ä»¶å­˜åœ¨")
        print("2. ç¡®ä¿æœ‰å†™å…¥æƒé™")
        print("3. å°è¯•è¿è¡Œ: mkdir -p result/logistic model_checkpoint/logistic")
